{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":4548890,"sourceType":"datasetVersion","datasetId":2655836}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers datasets","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport json\nimport numpy as np\nimport pandas as pd\nimport ast\nfrom transformers import AutoTokenizer\nimport math\nimport os\nfrom typing import Dict, List, Tuple, Optional\nfrom tqdm import tqdm\nimport re\n\nclass DistilBertConfig:\n    def __init__(self):\n        self.vocab_size = 30522\n        self.max_position_embeddings = 512\n        self.sinusoidal_pos_embds = False\n        self.n_layers = 6\n        self.n_heads = 12\n        self.dim = 768\n        self.hidden_dim = 3072\n        self.dropout = 0.1\n        self.attention_dropout = 0.1\n        self.activation = 'gelu'\n        self.initializer_range = 0.02\n        self.qa_dropout = 0.1\n\nclass MultiHeadSelfAttention(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.n_heads = config.n_heads\n        self.dim = config.dim\n        self.dropout = nn.Dropout(p=config.attention_dropout)\n        \n        assert self.dim % self.n_heads == 0\n        self.head_dim = self.dim // self.n_heads\n        \n        self.q_lin = nn.Linear(config.dim, config.dim)\n        self.k_lin = nn.Linear(config.dim, config.dim)\n        self.v_lin = nn.Linear(config.dim, config.dim)\n        self.out_lin = nn.Linear(config.dim, config.dim)\n        \n    def forward(self, query, key, value, mask=None):\n        batch_size, seq_len, dim = query.size()\n\n        q = self.q_lin(query).view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)\n        k = self.k_lin(key).view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)\n        v = self.v_lin(value).view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)\n\n        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n\n        if mask is not None:\n            mask = mask.unsqueeze(1).unsqueeze(2)\n            scores.masked_fill_(mask == 0, -1e9)\n\n        attn_weights = F.softmax(scores, dim=-1)\n        attn_weights = self.dropout(attn_weights)\n\n        context = torch.matmul(attn_weights, v)\n        context = context.transpose(1, 2).contiguous().view(batch_size, seq_len, dim)\n \n        output = self.out_lin(context)\n        return output\n\nclass FFN(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.lin1 = nn.Linear(config.dim, config.hidden_dim)\n        self.lin2 = nn.Linear(config.hidden_dim, config.dim)\n        self.dropout = nn.Dropout(p=config.dropout)\n        self.activation = nn.GELU()\n        \n    def forward(self, x):\n        x = self.lin1(x)\n        x = self.activation(x)\n        x = self.dropout(x)\n        x = self.lin2(x)\n        return x\n\nclass TransformerBlock(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.attention = MultiHeadSelfAttention(config)\n        self.sa_layer_norm = nn.LayerNorm(config.dim, eps=1e-12)\n        self.ffn = FFN(config)\n        self.output_layer_norm = nn.LayerNorm(config.dim, eps=1e-12)\n        self.dropout = nn.Dropout(p=config.dropout)\n        \n    def forward(self, x, attn_mask=None):\n        sa_output = self.attention(x, x, x, mask=attn_mask)\n        sa_output = self.dropout(sa_output)\n        sa_output = self.sa_layer_norm(sa_output + x)\n\n        ffn_output = self.ffn(sa_output)\n        ffn_output = self.dropout(ffn_output)\n        output = self.output_layer_norm(ffn_output + sa_output)\n        \n        return output\n\nclass DistilBertModel(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n\n        self.embeddings = nn.Embedding(config.vocab_size, config.dim, padding_idx=0)\n        if not config.sinusoidal_pos_embds:\n            self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.dim)\n\n        self.transformer = nn.ModuleList([\n            TransformerBlock(config) for _ in range(config.n_layers)\n        ])\n        \n        self.dropout = nn.Dropout(p=config.dropout)\n\n        self.init_weights()\n        \n    def init_weights(self):\n        for module in self.modules():\n            if isinstance(module, (nn.Linear, nn.Embedding)):\n                module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if isinstance(module, nn.Linear) and module.bias is not None:\n                module.bias.data.zero_()\n                \n    def get_position_embeddings(self, seq_len, device):\n        if self.config.sinusoidal_pos_embds:\n            position = torch.arange(seq_len, device=device).unsqueeze(1).float()\n            div_term = torch.exp(torch.arange(0, self.config.dim, 2, device=device).float() * \n                               (-math.log(10000.0) / self.config.dim))\n            pos_emb = torch.zeros(seq_len, self.config.dim, device=device)\n            pos_emb[:, 0::2] = torch.sin(position * div_term)\n            pos_emb[:, 1::2] = torch.cos(position * div_term)\n            return pos_emb\n        else:\n            positions = torch.arange(seq_len, device=device)\n            return self.position_embeddings(positions)\n    \n    def forward(self, input_ids, attention_mask=None):\n        batch_size, seq_len = input_ids.size()\n        device = input_ids.device\n\n        embeddings = self.embeddings(input_ids)\n\n        position_embeddings = self.get_position_embeddings(seq_len, device)\n        embeddings += position_embeddings\n        \n        embeddings = self.dropout(embeddings)\n        \n        hidden_state = embeddings\n        for layer in self.transformer:\n            hidden_state = layer(hidden_state, attention_mask)\n            \n        return hidden_state\n\nclass DistilBertForQuestionAnswering(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.distilbert = DistilBertModel(config)\n        self.qa_outputs = nn.Linear(config.dim, 2)  \n        self.dropout = nn.Dropout(p=config.qa_dropout)\n        \n    def forward(self, input_ids, attention_mask=None, start_positions=None, end_positions=None):\n        sequence_output = self.distilbert(input_ids, attention_mask)\n        sequence_output = self.dropout(sequence_output)\n        \n        logits = self.qa_outputs(sequence_output)\n        start_logits, end_logits = logits.split(1, dim=-1)\n        start_logits = start_logits.squeeze(-1)\n        end_logits = end_logits.squeeze(-1)\n        \n        outputs = {\n            'start_logits': start_logits,\n            'end_logits': end_logits\n        }\n        \n        if start_positions is not None and end_positions is not None:\n            ignored_index = start_logits.size(1)\n            start_positions.clamp_(0, ignored_index)\n            end_positions.clamp_(0, ignored_index)\n            \n            loss_fct = nn.CrossEntropyLoss(ignore_index=ignored_index)\n            start_loss = loss_fct(start_logits, start_positions)\n            end_loss = loss_fct(end_logits, end_positions)\n            total_loss = (start_loss + end_loss) / 2\n            outputs['loss'] = total_loss\n            \n        return outputs\n\nclass SQuADDataset(Dataset):\n    def __init__(self, data_path, tokenizer, max_length=512):\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        self.examples = []\n\n        print(f\"Loading data from {data_path}\")\n        df = pd.read_csv(data_path)\n        print(f\"Loaded {len(df)} rows from CSV\")\n\n        print(\"First row sample:\")\n        print(df.head(1).to_dict('records'))\n        \n        successful_parses = 0\n        failed_parses = 0\n        \n        for idx, row in df.iterrows():\n            try:\n                answers_str = str(row['answers'])\n\n                if idx < 3:\n                    print(f\"Row {idx} answers format: {answers_str}\")\n                \n                answer_text = None\n                answer_start = None\n\n                answer_text, answer_start = self._parse_answers(answers_str)\n \n                if answer_text is not None and answer_start is not None:\n                    answer_end = answer_start + len(answer_text)\n                    \n                    self.examples.append({\n                        'id': str(row['id']),\n                        'question': str(row['question']),\n                        'context': str(row['context']),\n                        'answer_text': answer_text,\n                        'answer_start': answer_start,\n                        'answer_end': answer_end\n                    })\n                    successful_parses += 1\n                else:\n                    failed_parses += 1\n                    if failed_parses <= 5:  \n                        print(f\"Failed to parse row {idx}: {answers_str}\")\n                    \n            except Exception as e:\n                failed_parses += 1\n                if failed_parses <= 5: \n                    print(f\"Error parsing row {idx}: {e}\")\n                continue\n        \n        print(f\"Successfully parsed {successful_parses} examples\")\n        print(f\"Failed to parse {failed_parses} examples\")\n        print(f\"Total examples in dataset: {len(self.examples)}\")\n        \n        if len(self.examples) == 0:\n            raise ValueError(\"No examples could be parsed from the dataset. Please check the data format.\")\n    \n    def _parse_answers(self, answers_str):\n        \"\"\"Enhanced parsing function for the answers column\"\"\"\n        try:\n            text_pattern = r\"'text':\\s*array\\(\\[([^\\]]+)\\],\\s*dtype=object\\)\"\n            start_pattern = r\"'answer_start':\\s*array\\(\\[([^\\]]+)\\],\\s*dtype=int32\\)\"\n            \n            text_match = re.search(text_pattern, answers_str)\n            start_match = re.search(start_pattern, answers_str)\n            \n            if text_match and start_match:\n                text_content = text_match.group(1)\n                start_content = start_match.group(1)\n\n                text_items = re.findall(r\"'([^']*)'|\\\"([^\\\"]*)\\\"\", text_content)\n                if text_items:\n                    answer_text = text_items[0][0] or text_items[0][1]\n                else:\n                    return None, None\n\n                start_items = re.findall(r'\\d+', start_content)\n                if start_items:\n                    answer_start = int(start_items[0])\n                else:\n                    return None, None\n                \n                return answer_text, answer_start\n\n            try:\n                cleaned_str = answers_str.replace('array(', '[').replace(', dtype=object)', ']').replace(', dtype=int32)', ']')\n\n                parsed_dict = ast.literal_eval(cleaned_str)\n                \n                if 'text' in parsed_dict and 'answer_start' in parsed_dict:\n                    text_list = parsed_dict['text']\n                    start_list = parsed_dict['answer_start']\n                    \n                    if isinstance(text_list, list) and len(text_list) > 0:\n                        answer_text = text_list[0]\n                    else:\n                        answer_text = text_list\n                    \n                    if isinstance(start_list, list) and len(start_list) > 0:\n                        answer_start = start_list[0]\n                    else:\n                        answer_start = start_list\n                    \n                    return answer_text, int(answer_start)\n            except:\n                pass\n\n            if \"'text':\" in answers_str and \"'answer_start':\" in answers_str:\n                text_start_idx = answers_str.find(\"'text':\") + len(\"'text':\")\n                text_section = answers_str[text_start_idx:]\n                \n                if \"array([\" in text_section:\n                    array_start = text_section.find(\"array([\") + len(\"array([\")\n                    array_end = text_section.find(\"]\", array_start)\n                    if array_end > array_start:\n                        text_array_content = text_section[array_start:array_end]\n\n                        text_matches = re.findall(r\"'([^']*)'|\\\"([^\\\"]*)\\\"\", text_array_content)\n                        if text_matches:\n                            answer_text = text_matches[0][0] or text_matches[0][1]\n                        else:\n                            return None, None\n                \n                start_start_idx = answers_str.find(\"'answer_start':\") + len(\"'answer_start':\")\n                start_section = answers_str[start_start_idx:]\n                \n                if \"array([\" in start_section:\n                    array_start = start_section.find(\"array([\") + len(\"array([\")\n                    array_end = start_section.find(\"]\", array_start)\n                    if array_end > array_start:\n                        start_array_content = start_section[array_start:array_end]\n\n                        start_matches = re.findall(r'\\d+', start_array_content)\n                        if start_matches:\n                            answer_start = int(start_matches[0])\n                            return answer_text, answer_start\n            \n            return None, None\n            \n        except Exception as e:\n            print(f\"Error in _parse_answers: {e}\")\n            return None, None\n    \n    def __len__(self):\n        return len(self.examples)\n    \n    def __getitem__(self, idx):\n        example = self.examples[idx]\n        \n        encoding = self.tokenizer(\n            example['question'],\n            example['context'],\n            max_length=self.max_length,\n            padding='max_length',\n            truncation=True,\n            return_tensors='pt',\n            return_offsets_mapping=True\n        )\n        \n        offset_mapping = encoding['offset_mapping'].squeeze()\n        start_positions = 0\n        end_positions = 0\n        \n        for i, (start_offset, end_offset) in enumerate(offset_mapping):\n            if start_offset <= example['answer_start'] < end_offset:\n                start_positions = i\n            if start_offset < example['answer_end'] <= end_offset:\n                end_positions = i\n                break\n        \n        return {\n            'input_ids': encoding['input_ids'].squeeze(),\n            'attention_mask': encoding['attention_mask'].squeeze(),\n            'start_positions': torch.tensor(start_positions, dtype=torch.long),\n            'end_positions': torch.tensor(end_positions, dtype=torch.long)\n        }\n\ndef load_pretrained_weights(model, pretrained_model_name='distilbert-base-uncased'):\n    \"\"\"Load pretrained DistilBERT weights from transformers library\"\"\"\n    try:\n        from transformers import DistilBertModel\n        pretrained_model = DistilBertModel.from_pretrained(pretrained_model_name)\n\n        model_dict = model.state_dict()\n        pretrained_dict = {}\n        \n        for name, param in pretrained_model.named_parameters():\n            if name.startswith('transformer.layer.'):\n                new_name = name.replace('transformer.layer.', 'distilbert.transformer.')\n            elif name.startswith('embeddings.'):\n                if 'word_embeddings' in name:\n                    new_name = name.replace('embeddings.word_embeddings', 'distilbert.embeddings')\n                elif 'position_embeddings' in name:\n                    new_name = name.replace('embeddings.position_embeddings', 'distilbert.position_embeddings')\n                else:\n                    continue\n            else:\n                new_name = f'distilbert.{name}'\n            \n            if new_name in model_dict and param.shape == model_dict[new_name].shape:\n                pretrained_dict[new_name] = param\n        \n        model_dict.update(pretrained_dict)\n        model.load_state_dict(model_dict)\n        print(f\"Loaded {len(pretrained_dict)} pretrained parameters\")\n        \n    except ImportError:\n        print(\"transformers library not found. Training from scratch.\")\n    except Exception as e:\n        print(f\"Error loading pretrained weights: {e}\")\n\n\ndef train_model(model, train_loader, val_loader, epochs=3, lr=2e-5):\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    model.to(device)\n    print(f\"Training on device: {device}\")\n    \n    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n    scheduler = torch.optim.lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor=0.1, total_iters=epochs)\n\n    os.makedirs('checkpoints', exist_ok=True)\n    \n    model.train()\n    for epoch in range(epochs):\n        print(f\"\\nEpoch {epoch+1}/{epochs}\")\n\n        total_loss = 0\n        train_pbar = tqdm(train_loader, desc=f\"Training Epoch {epoch+1}\", unit=\"batch\")\n        \n        for batch_idx, batch in enumerate(train_pbar):\n\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            start_positions = batch['start_positions'].to(device)\n            end_positions = batch['end_positions'].to(device)\n            \n            optimizer.zero_grad()\n\n            outputs = model(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                start_positions=start_positions,\n                end_positions=end_positions\n            )\n            \n            loss = outputs['loss']\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n            \n            total_loss += loss.item()\n\n            train_pbar.set_postfix({\n                'Loss': f'{loss.item():.4f}',\n                'Avg Loss': f'{total_loss/(batch_idx+1):.4f}'\n            })\n\n        model.eval()\n        val_loss = 0\n        val_pbar = tqdm(val_loader, desc=f\"Validation Epoch {epoch+1}\", unit=\"batch\")\n        \n        with torch.no_grad():\n            for batch in val_pbar:\n                input_ids = batch['input_ids'].to(device)\n                attention_mask = batch['attention_mask'].to(device)\n                start_positions = batch['start_positions'].to(device)\n                end_positions = batch['end_positions'].to(device)\n                \n                outputs = model(\n                    input_ids=input_ids,\n                    attention_mask=attention_mask,\n                    start_positions=start_positions,\n                    end_positions=end_positions\n                )\n                \n                val_loss += outputs['loss'].item()\n                val_pbar.set_postfix({'Val Loss': f'{outputs[\"loss\"].item():.4f}'})\n        \n        model.train()\n        scheduler.step()\n        \n        avg_train_loss = total_loss / len(train_loader)\n        avg_val_loss = val_loss / len(val_loader) if len(val_loader) > 0 else 0\n\n        checkpoint = {\n            'epoch': epoch + 1,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'scheduler_state_dict': scheduler.state_dict(),\n            'train_loss': avg_train_loss,\n            'val_loss': avg_val_loss,\n        }\n        \n        checkpoint_path = f'checkpoints/distilbert_qa_epoch_{epoch+1}.pth'\n        torch.save(checkpoint, checkpoint_path)\n        \n        print(f'Epoch {epoch+1} completed - Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}')\n        print(f'Checkpoint saved: {checkpoint_path}')\n    \n    print(\"\\nTraining completed!\")\n\ndef main():\n    config = DistilBertConfig()\n    model = DistilBertForQuestionAnswering(config)\n\n    load_pretrained_weights(model, 'distilbert-base-uncased')\n\n    tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n\n    train_dataset = SQuADDataset('/kaggle/input/the-stanford-question-answering-dataset/train.csv', tokenizer)\n\n    try:\n        val_dataset = SQuADDataset('/kaggle/input/the-stanford-question-answering-dataset/validation.csv', tokenizer)\n    except FileNotFoundError:\n\n        from torch.utils.data import random_split\n        train_size = int(0.9 * len(train_dataset))\n        val_size = len(train_dataset) - train_size\n        train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n\n    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n    \n    print(f\"Training examples: {len(train_dataset)}\")\n    print(f\"Validation examples: {len(val_dataset)}\")\n\n    train_model(model, train_loader, val_loader, epochs=10, lr=2e-5)\n\n    torch.save(model.state_dict(), 'distilbert_qa_model.pth')\n    print(\"Model saved successfully!\")\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer\nimport pandas as pd\nimport re\nimport random\n\ndef load_trained_model(model_path='distilbert_qa_model.pth'):\n    \"\"\"Load the trained DistilBERT QA model\"\"\"\n    config = DistilBertConfig()\n    model = DistilBertForQuestionAnswering(config)\n    \n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    model.load_state_dict(torch.load(model_path, map_location=device))\n    model.to(device)\n    model.eval()\n    \n    print(f\"Model loaded successfully from {model_path}\")\n    print(f\"Running on device: {device}\")\n    \n    return model, device\n\ndef load_contexts_from_csv(csv_path='/kaggle/input/the-stanford-question-answering-dataset/train.csv'):\n    \"\"\"Load all contexts from the training CSV file\"\"\"\n    print(f\"Loading contexts from {csv_path}\")\n    df = pd.read_csv(csv_path)\n\n    contexts = df['context'].drop_duplicates().tolist()\n    titles = df['title'].drop_duplicates().tolist()\n\n    context_mapping = {}\n    for _, row in df.iterrows():\n        context_mapping[row['context']] = row['title']\n    \n    print(f\"Loaded {len(contexts)} unique contexts from {len(df)} total examples\")\n    return contexts, context_mapping\n\ndef find_relevant_context(question, contexts, context_mapping, top_k=3):\n    \"\"\"Find the most relevant context for a given question using simple keyword matching\"\"\"\n    question_words = set(question.lower().split())\n    \n    context_scores = []\n    for context in contexts:\n        context_words = set(context.lower().split())\n\n        overlap = len(question_words.intersection(context_words))\n        relevance_score = overlap / len(question_words) if question_words else 0\n        \n        context_scores.append({\n            'context': context,\n            'title': context_mapping.get(context, 'Unknown'),\n            'score': relevance_score\n        })\n\n    context_scores.sort(key=lambda x: x['score'], reverse=True)\n    return context_scores[:top_k]\n\ndef answer_question(model, tokenizer, question, context, device, max_length=512):\n    \"\"\"Answer a question given a context using the trained model\"\"\"\n    inputs = tokenizer(\n        question,\n        context,\n        max_length=max_length,\n        padding='max_length',\n        truncation=True,\n        return_tensors='pt',\n        return_offsets_mapping=True\n    )\n    \n    input_ids = inputs['input_ids'].to(device)\n    attention_mask = inputs['attention_mask'].to(device)\n    offset_mapping = inputs['offset_mapping']\n    \n    with torch.no_grad():\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n        start_logits = outputs['start_logits']\n        end_logits = outputs['end_logits']\n    \n    start_idx = torch.argmax(start_logits, dim=1).item()\n    end_idx = torch.argmax(end_logits, dim=1).item()\n    \n    if end_idx < start_idx:\n        end_idx = start_idx\n    \n    start_confidence = torch.softmax(start_logits, dim=1)[0, start_idx].item()\n    end_confidence = torch.softmax(end_logits, dim=1)[0, end_idx].item()\n    confidence = (start_confidence + end_confidence) / 2\n    \n    answer_tokens = input_ids[0, start_idx:end_idx+1]\n    answer = tokenizer.decode(answer_tokens, skip_special_tokens=True)\n    \n    if len(offset_mapping[0]) > start_idx and len(offset_mapping[0]) > end_idx:\n        start_char = offset_mapping[0][start_idx][0].item()\n        end_char = offset_mapping[0][end_idx][1].item()\n        \n        sep_token_idx = (input_ids[0] == tokenizer.sep_token_id).nonzero(as_tuple=True)[0]\n        if len(sep_token_idx) > 0:\n            context_start_token = sep_token_idx[0].item() + 1\n            if start_idx >= context_start_token:\n                context_offset = offset_mapping[0][context_start_token][0].item()\n                start_char_in_context = start_char - context_offset\n                end_char_in_context = end_char - context_offset\n                \n                if start_char_in_context >= 0 and end_char_in_context <= len(context):\n                    answer_from_context = context[start_char_in_context:end_char_in_context]\n                    if answer_from_context.strip():\n                        answer = answer_from_context.strip()\n    \n    return {\n        'answer': answer,\n        'confidence': confidence,\n        'context_title': None \n    }\n\nprint(\"Loading trained model and tokenizer...\")\nmodel, device = load_trained_model('distilbert_qa_model.pth')\ntokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n\nprint(\"\\nLoading contexts from training data...\")\ncontexts, context_mapping = load_contexts_from_csv()\n\n\nquestion = \"Which NFL team represented the AFC at Super Bowl 50?\"\n\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"QUESTION ANSWERING WITH AUTOMATIC CONTEXT SELECTION\")\nprint(\"=\"*60)\nprint(f\"Question: {question}\")\n\n\nrelevant_contexts = find_relevant_context(question, contexts, context_mapping, top_k=3)\n\nprint(f\"\\nTop {len(relevant_contexts)} most relevant contexts found:\")\nfor i, ctx_info in enumerate(relevant_contexts, 1):\n    print(f\"{i}. {ctx_info['title']} (relevance: {ctx_info['score']:.3f})\")\n\nif relevant_contexts:\n    best_context = relevant_contexts[0]\n    result = answer_question(model, tokenizer, question, best_context['context'], device)\n    result['context_title'] = best_context['title']\n    \n    print(f\"\\n{'='*60}\")\n    print(\"ANSWER\")\n    print(\"=\"*60)\n    print(f\"ðŸŽ¯ {result['answer']}\")\n    print(f\"ðŸ“Š Confidence: {result['confidence']:.2%}\")\n    print(f\"ðŸ“š Source: {result['context_title']}\")\n    print(f\"ðŸ” Context relevance: {best_context['score']:.3f}\")\n\n    context_preview = best_context['context'][:200] + \"...\" if len(best_context['context']) > 200 else best_context['context']\n    print(f\"\\nðŸ“– Context used: {context_preview}\")\n    \nelse:\n    print(\"âŒ No relevant context found for this question.\")\n\n\nprint(f\"\\n{'='*50}\")\nprint(\"ðŸŽ² RANDOM EXAMPLES FROM YOUR TRAINING DATA:\")\nprint(\"=\"*50)\nsample_contexts = random.sample(list(context_mapping.items()), min(3, len(context_mapping)))\nfor context, title in sample_contexts:\n    print(f\"ðŸ“š {title}\")\n    preview = context[:150] + \"...\" if len(context) > 150 else context\n    print(f\"   {preview}\\n\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}